---
layout: post
title: A Practical Guide To Off Policy Evaluation
date: 07-06-2023 16:27:43
tags: [stats]
description:
summary:
comments: false
author: Yong Huang
mathjax: true
render_with_liquid: true
---

## What is Off Policy Evaluation

Off Policy Evaluation (OPE) is a critical component in the field of reinforcement learning and decision making. It involves assessing the performance of a given policy using data that was generated by a different policy. This is especially important in scenarios where deploying a new policy to gather data is expensive, risky, or impractical.

## Notations

To facilitate the understanding of various OPE methods and concepts discussed in this blog, here are a list of notations used throughout the text:

- \( N \): Number of samples or trajectories.
- \( T \): Length of a decision horizon or trajectory.
- \( \pi(a | s) \): Probability of taking action \( a \) in state \( s \) under the evaluation policy.
- \( \mu(a | s) \): Probability of taking action \( a \) in state \( s \) under the behavior policy.
- \( s_i \): State at the \( i \)-th sample.
- \( a_i \): Action taken at the \( i \)-th sample.
- \( r_i \): Reward observed at the \( i \)-th sample.
- \( \hat{r}(s, a) \): Predicted reward for state \( s \) and action \( a \) from a model.
- \( \gamma \): Discount factor used in sequential decision settings to account for the time value of rewards.
- \( \mathbb{E}[r | s, a] \): Expected reward given state \( s \) and action \( a \).


## OPE Methods in Bandit Settings

### A Toy Dataset

To illustrate OPE methods in bandit settings, we can consider a simple toy dataset. This dataset can be generated from a bandit problem with a known reward distribution.

blablabla

### Importance Sampling

Importance Sampling (IS) is a popular method for OPE in bandit settings. It reweights the observed rewards according to the ratio of the probabilities of actions under the evaluation policy and the behavior policy. Formally, the IS estimator is given by:

$$
\hat{V}_{\text{IS}} = \frac{1}{N} \sum_{i=1}^N \frac{\pi(a_i | s_i)}{\mu(a_i | s_i)} r_i
$$

### Direct Methods

Direct Methods (DM) involve fitting a model to predict the expected reward for each action and then using this model to estimate the value of the evaluation policy. The DM estimator can be expressed as:

$$
\hat{V}_{\text{DM}} = \frac{1}{N} \sum_{i=1}^N \mathbb{E}[r | s_i, a_i]
$$

### Doubly Robust

The Doubly Robust (DR) method combines the strengths of IS and DM by using a model to correct the bias in the IS estimator, making it more robust to model misspecification. The DR estimator is defined as:

$$
\hat{V}_{\text{DR}} = \frac{1}{N} \sum_{i=1}^N \left( \frac{\pi(a_i | s_i)}{\mu(a_i | s_i)} (r_i - \hat{r}(s_i, a_i)) + \hat{r}(s_i, a_i) \right)
$$

## OPE Methods in Sequential Decision Settings

### Another Toy Dataset

In sequential decision settings, we use a different toy dataset that involves sequences of decisions and rewards. This dataset can be generated from a Markov Decision Process (MDP).

### Importance Sampling

In sequential decision settings, IS can be extended to account for the sequential nature of the data. This involves reweighting the entire trajectory of actions and rewards. The IS estimator for sequential settings is:

$$
\hat{V}_{\text{IS, seq}} = \frac{1}{N} \sum_{i=1}^N \prod_{t=1}^T \frac{\pi(a_{i,t} | s_{i,t})}{\mu(a_{i,t} | s_{i,t})} r_{i,t}
$$

### Direct Methods

Direct Methods in sequential settings involve learning a model that predicts the expected cumulative reward for each state-action pair, which is then used to estimate the policy's value. The DM estimator for sequential settings is:

$$
\hat{V}_{\text{DM, seq}} = \frac{1}{N} \sum_{i=1}^N \sum_{t=1}^T \gamma^t \mathbb{E}[r_{i,t} | s_{i,t}, a_{i,t}]
$$

### Doubly Robust

The DR method can also be applied in sequential settings, combining IS with a predictive model to correct for bias and improve the estimator's robustness. The DR estimator for sequential settings is:

$$
\hat{V}_{\text{DR, seq}} = \frac{1}{N} \sum_{i=1}^N \sum_{t=1}^T \gamma^t \left( \frac{\pi(a_{i,t} | s_{i,t})}{\mu(a_{i,t} | s_{i,t})} (r_{i,t} - \hat{r}(s_{i,t}, a_{i,t})) + \hat{r}(s_{i,t}, a_{i,t}) \right)
$$

## Some Important Notions of OPE Estimator

### Bias

Bias refers to the systematic error introduced by an estimator. In the context of OPE, it is important to understand and mitigate bias to ensure accurate evaluation.

### Variance

Variance measures the variability of the estimator's predictions. High variance can lead to unreliable estimates, making it crucial to balance bias and variance.

### Efficiency Bounds

Efficiency bounds define the best possible performance of an estimator in terms of bias and variance. Understanding these bounds helps in assessing the quality of different OPE methods.

## Why All OPEs Are Practically Biased?

### Curse of Horizon

The curse of horizon refers to the exponential growth in the number of possible trajectories with the length of the decision horizon, making it difficult to obtain accurate estimates.

### Model-Free Methods Aren't Really Model-Free

Many so-called model-free methods still rely on implicit modeling assumptions, which can introduce bias into the evaluation.

### Identifiabitity

### Estimatability


## What You Should Care Before Using OPE?

#### Horizon

#### How much confounding?

#### 

## A Practical Flowchart

